{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import json\n",
    "import h5py\n",
    "from utils import img_data_2_mini_batch, imgs2batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_f = 'cocoqa_data_prepro_'\n",
    "base_n = '93'\n",
    "base_fn = base_f + base_n\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_h5 = h5py.File(base_fn+'.h5', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_json = json.load(open(base_fn+'.json', 'r'))\n",
    "# pad fix\n",
    "itow = val_data_json['ix_to_word']\n",
    "\n",
    "wtoi = {iv: ik for ik,iv in itow.iteritems()}\n",
    "old_pad = wtoi['<pad>']\n",
    "wtoi['<pad_fix>'] = old_pad\n",
    "wtoi['<pad>'] = '0'\n",
    "itow[old_pad] = '<pad_fix>'\n",
    "itow['0'] = '<pad>'\n",
    "\n",
    "\n",
    "assert(wtoi['<pad>'] == '0')\n",
    "assert(itow['0'] == '<pad>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itoa = val_data_json['ix_to_ans']\n",
    "\n",
    "unique_img_val = val_data_json['unique_img_val']\n",
    "ques_val = val_data_h5['ques_val'][:]\n",
    "ans_val = val_data_h5['ans_val'][:]\n",
    "question_id_val = val_data_h5['question_id_val'][:]\n",
    "img_pos_val = val_data_h5['img_pos_val'][:]\n",
    "images = np.array(imgs2batch(unique_img_val, img_pos_val, transform=transform))\n",
    "ques_val = np.array(ques_val)\n",
    "ans_val = np.array(ans_val).reshape((-1, 1))\n",
    "\n",
    "images = torch.from_numpy(images)\n",
    "ques_val = torch.from_numpy(ques_val)\n",
    "ans_val = torch.from_numpy(ans_val)\n",
    "\n",
    "# for i in range(ques_val.size(1)):\n",
    "#     i += 60\n",
    "    \n",
    "#     _img = images[i]\n",
    "#     _img = _img.detach().numpy()\n",
    "#     plt.figure()\n",
    "#     plt.imshow(_img)\n",
    "#     print 'Question: ' + ' '.join(filter(lambda kx: kx!='<pad>',(map(lambda wr: itow[str(wr)], ques_val[i].detach().numpy().tolist()))))\n",
    "#     print 'Answer: ' + ' '.join(map(lambda wr: itoa[str(wr)], ans_val[i].detach().numpy().tolist()))\n",
    "#     print \n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([84, 3, 224, 224]), torch.Size([84, 27]))\n",
      "(torch.Size([9, 3, 224, 224]), torch.Size([9, 27]))\n"
     ]
    }
   ],
   "source": [
    "ques_ans_val = torch.cat((ques_val, ans_val), dim=1)\n",
    "BATCH_SIZE = 20\n",
    "split_point = int(0.1 * ques_ans_val.size(0)) # split 10% for testing\n",
    "\n",
    "ques_ans_splits = torch.split(ques_ans_val, split_point, dim=0)\n",
    "images_splits = torch.split(images, split_point, dim=0)\n",
    "\n",
    "ques_ans_test = ques_ans_splits[0]\n",
    "ques_ans_train = torch.cat(ques_ans_splits[1:], dim=0)\n",
    "\n",
    "images_test = images_splits[0]\n",
    "images_train = torch.cat(images_splits[1:], dim=0)\n",
    "\n",
    "# should be (torch.Size([TRAIN_SIZE, 3, 224, 224]), torch.Size([TRAIN_SIZE, MAX_LENGTH]))\n",
    "print(images_train.size(), ques_ans_train.size()) \n",
    "# should be (torch.Size([TEST_SIZEZ, 3, 224, 224]), torch.Size([TEST_SIZE, MAX_LENGTH]))\n",
    "print(images_test.size(), ques_ans_test.size())\n",
    "\n",
    "train_dataset=Data.TensorDataset(images_train, ques_ans_train)\n",
    "test_dataset=Data.TensorDataset(images_test, ques_ans_test)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed 8 hidden 8 ques_vocab 212 ans_vocab 39\n"
     ]
    }
   ],
   "source": [
    "# from naive import Enc, Dec\n",
    "from att_model import Enc, Dec\n",
    "device = torch.device('cpu')\n",
    "embed_size = 8\n",
    "hidden_size = 8\n",
    "ques_vocab_size = len(itow)\n",
    "ans_vocab_size = len(itoa)+1\n",
    "num_layers = 1\n",
    "\n",
    "print 'embed',embed_size,'hidden',hidden_size,'ques_vocab',ques_vocab_size, 'ans_vocab',ans_vocab_size\n",
    "encoder = Enc(embed_size).to(device)\n",
    "decoder = Dec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, num_layers)\n",
    "# encoder.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec(\n",
      "  (language_model): LanguageModel(\n",
      "    (embed): Embedding(212, 8)\n",
      "    (lstm): LSTM(8, 8, batch_first=True)\n",
      "    (linear): Linear(in_features=8, out_features=39, bias=True)\n",
      "  )\n",
      "  (attention_model): AttentionModel(\n",
      "    (conv1): Conv2d(2048, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (conv2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (drop1): Dropout(p=0.0)\n",
      "    (relu): ReLU(inplace)\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (drop1): Dropout(p=0.5)\n",
      "    (lin1): Linear(in_features=4104, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (drop2): Dropout(p=0.5)\n",
      "    (lin2): Linear(in_features=512, out_features=39, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "att_model.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention = F.softmax(attention)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2886: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# optimizer and loss\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=0.01)\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# start your train\n",
    "lossList = []\n",
    "accList = []\n",
    "for epoch in range(100):\n",
    "    for i, (images, img_ans_val) in enumerate(train_loader):\n",
    "        ques, ans = torch.split(img_ans_val, 26,dim=1)\n",
    "        # images of shape [batch, 3, 256, 256]\n",
    "        # ques of shape [batch, 26]\n",
    "        # ans of shape [batch, 1]\n",
    "\n",
    "        lengths = []\n",
    "        for qix in ques:\n",
    "            for iy in range(len(qix)):\n",
    "                if (qix[iy]==0):\n",
    "                    lengths.append(iy)\n",
    "                    break;\n",
    "        tups = []\n",
    "        for ix in range(ques.size(0)):\n",
    "            row = ques[ix,:]\n",
    "            length = lengths[ix]\n",
    "            image_i = images[ix,:]\n",
    "            ans_i = ans[ix,:]\n",
    "            tup = (row, length, image_i, ans_i)\n",
    "            tups.append(tup)\n",
    "\n",
    "        sorted_tuples = sorted(tups, key=lambda tup: tup[1], reverse=True)\n",
    "        questions = torch.stack(list(map(lambda tup: tup[0], sorted_tuples)))\n",
    "        images = torch.stack(list(map(lambda tup: tup[2], sorted_tuples)))\n",
    "        answers = torch.stack(list(map(lambda tup: tup[3], sorted_tuples)))\n",
    "        lengths = list(map(lambda tup: tup[1], sorted_tuples))\n",
    "    \n",
    "        images = images.to(device)\n",
    "        questions = questions.to(device).long()\n",
    "        raw_features, features = encoder(images)\n",
    "        output = decoder(raw_features, features, questions, lengths)\n",
    "        sys.exit()\n",
    "#         answers = answers.reshape((-1)).long()\n",
    "        \n",
    "#         loss = F.nll_loss(output, answers)\n",
    "        \n",
    "#         # copy here\n",
    "#         lossList.append(loss.item())\n",
    "        \n",
    "#         _, pred = torch.max(output, dim=1)\n",
    "        \n",
    "#         correct = pred.eq(answers.long().view_as(pred)).sum()\n",
    "#         acc = float(correct) / float(BATCH_SIZE)\n",
    "        \n",
    "#         accList.append(acc)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print('epoch',epoch,'#', i, 'loss:', loss.item(), 'acc:', acc, 'correct:', correct)\n",
    "#     break\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(range(len(lossList)), lossList, 'ro')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(accList)), accList, 'ro')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
