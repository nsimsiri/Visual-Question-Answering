{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from naive import Enc, Dec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 2, 3],\n",
      "        [7, 8, 9],\n",
      "        [4, 5, 0],\n",
      "        [6, 0, 0]]), tensor([3, 3, 2, 1]))\n",
      "('inputs size: ', torch.Size([3, 4]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5ca77e417893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# max_seq_len * batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedded size: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# max_seq_len * batch_size * emb_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import numpy as np\n",
    "\n",
    "max_seq_len = 3\n",
    "batch_size = 4\n",
    "layer_num = 2\n",
    "\n",
    "input_size = 10 # 0 - 9\n",
    "emb_size = 8\n",
    "hidden_size = 10\n",
    "\n",
    "# test data:\n",
    "a = [[1,2,3], [4,5,0], [6,0,0], [7,8,9]]\n",
    "lens = [3,2,1,3]\n",
    "\n",
    "# sort the input batch data by reversed actual length for pad_pack operation\n",
    "pairs = sorted( zip(a, lens), key=lambda p: p[1], reverse=True)\n",
    "(a, lens) = zip(*pairs)\n",
    "\n",
    "# actual length\n",
    "lens = np.array(lens)\n",
    "# lens = torch.LongTensor(lens)\n",
    "\n",
    "va = Variable(torch.LongTensor(a))\n",
    "vlens = Variable(torch.LongTensor(lens))\n",
    "\n",
    "print(va, vlens)\n",
    "\n",
    "embedding = nn.Embedding(input_size, emb_size)\n",
    "gru = nn.GRU(emb_size, hidden_size, layer_num)\n",
    "\n",
    "inputs = va.transpose(0, 1); print(\"inputs size: \", inputs.size()) # max_seq_len * batch_size\n",
    "inputs = embedding(inputs); print(\"embedded size: \", embedding.size()) # max_seq_len * batch_size * emb_size\n",
    "\n",
    "inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs, lens)\n",
    "\n",
    "h0 = Variable(torch.randn(layer_num, 4, 10))\n",
    "outputs, hn = gru(inputs, h0)\n",
    "\n",
    "# print(\"after packed:\")\n",
    "# print(\"outputs.size: \", outputs.size())\n",
    "# print(\"hn size: \", hn.size())\n",
    "\n",
    "tmp = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "\n",
    "outputs, output_lengths = tmp\n",
    "# outputs: (max_seq_len * batch_size, output_size)\n",
    "\n",
    "print(\"after padded:\")\n",
    "print(\"outputs.size: \", outputs.size())\n",
    "print(\"hn size: \", hn.size())\n",
    "\n",
    "# outputs.index_select(torch.LongTensor(lens-1))\n",
    "# print(\"lens: \", lens-1)\n",
    "\n",
    "# idxs = Variable(torch.LongTensor(lens-1))\n",
    "# outputs = outputs.index_select(0, idxs)\n",
    "\n",
    "# print(\"selected outputs size: \", outputs.size())\n",
    "# outputs.gather(0, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_IMG = \"COCO_val2014_000000000042.jpg\"\n",
    "# pil_im = Image.open('data/val2014/COCO_val2014_000000000042.jpg', 'r')\n",
    "# plt.figure()\n",
    "# plt.imshow(np.asarray(pil_im))\n",
    "\n",
    "# pil_im2= pil_im.resize((256, 256), Image.ANTIALIAS)\n",
    "# im2a = np.array(pil_im2)\n",
    "# # imshow(np.asarray(pil_im))\n",
    "# print im2a.shape\n",
    "# plt.figure()\n",
    "# plt.imshow(im2a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
