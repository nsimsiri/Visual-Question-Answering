{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from utils import imread, img_data_2_mini_batch, imgs2batch\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from naive import Enc, Dec\n",
    "from data_loader import VQADataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data_2000.pkl\n",
      "reading from ./data/data_2000.pkl\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "dataset_filename = \"./data/data_{}.pkl\".format(N)\n",
    "dataset = None\n",
    "print(dataset_filename)\n",
    "if (os.path.exists(dataset_filename)):\n",
    "    with open(dataset_filename, 'rb') as handle:\n",
    "        print(\"reading from \" + dataset_filename)\n",
    "        dataset = pickle.load(handle)\n",
    "else:\n",
    "    dataset = VQADataSet(Q=N)\n",
    "    with open(dataset_filename, 'wb') as handle:\n",
    "        print(\"writing to \" + dataset_filename)\n",
    "        pickle.dump(dataset, handle)\n",
    "\n",
    "assert(dataset is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size        = 128\n",
    "hidden_size       = 128\n",
    "batch_size        = 32\n",
    "ques_vocab_size   = len(dataset.vocab['question'])\n",
    "ans_vocab_size    = len(dataset.vocab['answer'])\n",
    "rnn_layers        = 1\n",
    "n_epochs          = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /home/nsimsiri/.torch/models/resnet152-b121ed2d.pth\n",
      "100%|██████████| 241530880/241530880 [00:03<00:00, 77313050.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Enc(embed_size).to(device)\n",
    "decoder = Dec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, rnn_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "print(\"device: {}\".format(device))\n",
    "# print(encoder)\n",
    "# print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader=None, batch_size=batch_size, epoch=1):\n",
    "    if data_loader is None:\n",
    "        return\n",
    "    for i, minibatch in enumerate(data_loader):\n",
    "        # extract minibatch\n",
    "        idxs, v, q, a, q_len = minibatch\n",
    "        \n",
    "        # convert torch's DataLoader output to proper format.\n",
    "        # torch gives a List[Tensor_1, ... ] where tensor has been transposed. \n",
    "        # batchify transposes back.\n",
    "        v = v.to(device)\n",
    "        q = VQADataSet.batchify_questions(q).to(device)\n",
    "        a = a.to(device)\n",
    "        print(\"\")\n",
    "#         print('V: ', v.shape)\n",
    "#         print('Q: ', q.shape)\n",
    "#         print('A: ', a.shape)\n",
    "\n",
    "        img_features = encoder(v)\n",
    "        print(\"img_features\", img_features.shape)\n",
    "\n",
    "        pred = decoder(img_features, q, q_len)\n",
    "        print(\"pred\", pred.shape)\n",
    "\n",
    "        loss = criterion(pred, a)          \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32 shuffle: True\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 0 loss: 7.162944316864014\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 1 loss: 7.1589789390563965\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 2 loss: 7.148438930511475\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 3 loss: 7.15594482421875\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 4 loss: 7.147489070892334\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 5 loss: 7.139472484588623\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 6 loss: 7.119172096252441\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 7 loss: 7.176149368286133\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 8 loss: 7.138587474822998\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 9 loss: 7.129648685455322\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 10 loss: 7.141820430755615\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 11 loss: 7.102893829345703\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 12 loss: 7.121181488037109\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 13 loss: 7.160431861877441\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 14 loss: 7.062370300292969\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 15 loss: 7.10170841217041\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 16 loss: 7.13786506652832\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 17 loss: 7.096205711364746\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 18 loss: 7.131991863250732\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 19 loss: 7.110184192657471\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 20 loss: 7.104800701141357\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 21 loss: 7.065319538116455\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 22 loss: 7.006026744842529\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 23 loss: 7.013533115386963\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 24 loss: 7.029341220855713\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 25 loss: 7.027054309844971\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 26 loss: 7.092758655548096\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 27 loss: 7.036288738250732\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 28 loss: 7.111891746520996\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 29 loss: 6.980937957763672\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 30 loss: 7.050634384155273\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 31 loss: 7.035155296325684\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 32 loss: 7.056229591369629\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 33 loss: 7.027186393737793\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 34 loss: 7.028640270233154\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 35 loss: 6.887939929962158\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 36 loss: 7.086997032165527\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 37 loss: 7.130434513092041\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 38 loss: 6.970574855804443\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 39 loss: 6.915374755859375\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 40 loss: 6.867015361785889\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 41 loss: 7.094902038574219\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 42 loss: 6.883996486663818\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 43 loss: 6.799215793609619\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 44 loss: 6.950824737548828\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 45 loss: 6.948217868804932\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 46 loss: 6.80955696105957\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 47 loss: 6.615548610687256\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 48 loss: 6.751607894897461\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 49 loss: 6.773430824279785\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 50 loss: 6.644820213317871\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 51 loss: 6.332332611083984\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 52 loss: 6.757726669311523\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 53 loss: 6.757348537445068\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 54 loss: 6.664491176605225\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 55 loss: 6.752447605133057\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 56 loss: 6.678677558898926\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 57 loss: 6.588140964508057\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 58 loss: 6.708078861236572\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 59 loss: 6.746767044067383\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 60 loss: 6.442636013031006\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 61 loss: 6.390973091125488\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 62 loss: 6.77535343170166\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 63 loss: 6.538381099700928\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 64 loss: 6.541701793670654\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 65 loss: 6.796393871307373\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 66 loss: 5.826407432556152\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 67 loss: 6.254202842712402\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 68 loss: 6.225494384765625\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 69 loss: 5.890608787536621\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 70 loss: 6.724818706512451\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 71 loss: 5.864489555358887\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 72 loss: 6.568077564239502\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 73 loss: 6.3741984367370605\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 74 loss: 5.707981586456299\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 75 loss: 6.658932685852051\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 76 loss: 6.270421981811523\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 77 loss: 6.221484184265137\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 78 loss: 6.326788902282715\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 79 loss: 6.194586277008057\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 # 80 loss: 6.863967418670654\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 81 loss: 6.5897650718688965\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 82 loss: 6.203444480895996\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 83 loss: 7.221890926361084\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 84 loss: 6.035122394561768\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 85 loss: 6.0488104820251465\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 86 loss: 6.487653732299805\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 87 loss: 6.602327823638916\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 88 loss: 5.422983169555664\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 89 loss: 6.330343246459961\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 90 loss: 6.385890483856201\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 91 loss: 5.971978664398193\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 92 loss: 5.9527482986450195\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 93 loss: 6.186298847198486\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 94 loss: 5.70001745223999\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 95 loss: 5.529672622680664\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 96 loss: 6.553884029388428\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 97 loss: 6.4002685546875\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 98 loss: 5.471018314361572\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 99 loss: 6.521317481994629\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 100 loss: 6.3102827072143555\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 101 loss: 6.299429416656494\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 102 loss: 6.200219631195068\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 103 loss: 6.37343168258667\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 104 loss: 5.305706977844238\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 105 loss: 6.345197677612305\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 106 loss: 6.417143821716309\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 107 loss: 6.497864246368408\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 108 loss: 6.294500350952148\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 109 loss: 5.379328727722168\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 110 loss: 5.874204635620117\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 111 loss: 5.276313304901123\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 112 loss: 5.983736038208008\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 113 loss: 6.230048656463623\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 114 loss: 5.85093879699707\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 115 loss: 6.2454071044921875\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 116 loss: 6.667575836181641\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 117 loss: 6.333312034606934\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 118 loss: 5.504030227661133\n",
      "\n",
      "img_features torch.Size([32, 128])\n",
      "pred torch.Size([32, 1282])\n",
      "epoch: 1 # 119 loss: 6.198315620422363\n",
      "\n",
      "img_features torch.Size([10, 128])\n",
      "pred torch.Size([10, 1282])\n",
      "epoch: 1 # 120 loss: 5.307509422302246\n"
     ]
    }
   ],
   "source": [
    "train_loader = dataset.build_data_loader(train=True, args={'batch_size': batch_size})\n",
    "\n",
    "eval_model(data_loader = train_loader, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
