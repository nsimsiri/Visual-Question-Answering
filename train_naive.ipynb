{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import plotting\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import imread, img_data_2_mini_batch, imgs2batch\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from naive import Enc, Dec, EncDec\n",
    "from data_loader import VQADataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data_2000.pkl\n",
      "reading from ./data/data_2000.pkl\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "dataset_filename = \"./data/data_{}.pkl\".format(N)\n",
    "dataset = None\n",
    "print(dataset_filename)\n",
    "if (os.path.exists(dataset_filename)):\n",
    "    with open(dataset_filename, 'rb') as handle:\n",
    "        print(\"reading from \" + dataset_filename)\n",
    "        dataset = pickle.load(handle)\n",
    "else:\n",
    "    dataset = VQADataSet(Q=N)\n",
    "    with open(dataset_filename, 'wb') as handle:\n",
    "        print(\"writing to \" + dataset_filename)\n",
    "        pickle.dump(dataset, handle)\n",
    "\n",
    "assert(dataset is not None)\n",
    "def debug(v,q,a):\n",
    "    print('\\nV: {}\\nQ: {}\\nA: {}'.format(v.shape, q.shape, a.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469 1282\n"
     ]
    }
   ],
   "source": [
    "embed_size        = 128\n",
    "hidden_size       = 128\n",
    "batch_size        = 32\n",
    "ques_vocab_size   = len(dataset.vocab['question'])\n",
    "ans_vocab_size    = len(dataset.vocab['answer'])\n",
    "rnn_layers        = 1\n",
    "n_epochs          = 25\n",
    "learning_rate     = 0.01\n",
    "momentum          = 0.98\n",
    "\n",
    "print(ques_vocab_size, ans_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, criterion, optimizer, batch_size, training=False,\n",
    "              total_loss_over_epochs=[], scores_over_epochs=[]):\n",
    "    running_loss = 0.\n",
    "    final_labels, final_preds = [], []\n",
    "    if data_loader is None:\n",
    "        return\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    for i, minibatch in enumerate(data_loader):\n",
    "        # extract minibatch\n",
    "        t0 = time.time()\n",
    "        idxs, v, q, a, q_len = minibatch\n",
    "        \n",
    "        # convert torch's DataLoader output to proper format.\n",
    "        # torch gives a List[Tensor_1, ... ] where tensor has been transposed. \n",
    "        # batchify transposes back.`\n",
    "        v = v.to(device)\n",
    "        q = VQADataSet.batchify_questions(q).to(device)\n",
    "        a = a.to(device)\n",
    "\n",
    "        logits = model(v, q, q_len)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        loss = F.nll_loss(logits, a)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        score = metrics.precision_recall_fscore_support(preds.tolist(),\n",
    "                                                        a.tolist(),\n",
    "                                                        average='weighted')\n",
    "        \n",
    "        total_loss_over_epochs['train_loss'].append(loss)\n",
    "        scores_over_epochs['train_scores'].append(score)\n",
    "        \n",
    "        if training and optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        final_labels += a.tolist()\n",
    "        final_preds  += preds.tolist()\n",
    "        if True:#(i%20==0):\n",
    "#             plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(7,3))\n",
    "#             plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(7, 3), title=\"Loss\")\n",
    "            print(\"Loss: {} - score: {} - t: {}\".format(loss, score, time.time()-t0))\n",
    "            \n",
    "    return running_loss, final_labels, final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EncDec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, rnn_layers).to(device)\n",
    "\n",
    "model = EncDec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, rnn_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.get_parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32 shuffle: True\n",
      "batch_size: 32 shuffle: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.14402961730957 - score: (0.0, 0.0, 0.0, None) - t: 0.26078081130981445\n",
      "Loss: 6.908522605895996 - score: (0.0234375, 0.03125, 0.026785714285714288, None) - t: 0.23926687240600586\n",
      "Loss: 6.672055721282959 - score: (0.59375, 0.125, 0.19949494949494945, None) - t: 0.2002110481262207\n",
      "Loss: 6.0676422119140625 - score: (0.6774553571428571, 0.15625, 0.2153897849462365, None) - t: 0.19977331161499023\n",
      "Loss: 5.618424415588379 - score: (0.44062500000000004, 0.15625, 0.23069221967963385, None) - t: 0.1987471580505371\n",
      "Loss: 5.6899800300598145 - score: (0.6805555555555556, 0.25, 0.3654411764705882, None) - t: 0.19895529747009277\n",
      "Loss: 6.164545059204102 - score: (1.0, 0.15625, 0.2702702702702703, None) - t: 0.19988417625427246\n",
      "Loss: 7.592854022979736 - score: (1.0, 0.03125, 0.06060606060606061, None) - t: 0.1997828483581543\n",
      "Loss: 7.332056999206543 - score: (1.0, 0.15625, 0.2702702702702703, None) - t: 0.20004916191101074\n",
      "Loss: 5.667515754699707 - score: (1.0, 0.21875, 0.358974358974359, None) - t: 0.19980835914611816\n",
      "Loss: 6.022980690002441 - score: (1.0, 0.25, 0.4, None) - t: 0.19902276992797852\n",
      "Loss: 6.472331523895264 - score: (0.875, 0.125, 0.21875, None) - t: 0.19970178604125977\n",
      "Loss: 6.0631103515625 - score: (0.84375, 0.09375, 0.16874999999999998, None) - t: 0.19942164421081543\n",
      "Loss: 6.079638481140137 - score: (0.78125, 0.125, 0.2155172413793104, None) - t: 0.19970035552978516\n",
      "Loss: 6.613470554351807 - score: (0.5833333333333333, 0.0625, 0.11290322580645161, None) - t: 0.21246933937072754\n",
      "Loss: 5.987253665924072 - score: (0.78125, 0.21875000000000003, 0.34179687500000006, None) - t: 0.20033740997314453\n",
      "Loss: 6.057969093322754 - score: (0.765625, 0.15625, 0.2561813186813187, None) - t: 0.19858145713806152\n",
      "Loss: 5.616159439086914 - score: (0.5177083333333333, 0.15625, 0.23841533180778035, None) - t: 0.19930481910705566\n",
      "Loss: 6.422367095947266 - score: (0.8500000000000001, 0.15625, 0.25560897435897434, None) - t: 0.19895625114440918\n",
      "Loss: 6.98220157623291 - score: (0.5, 0.03125, 0.058823529411764705, None) - t: 0.200242280960083\n",
      "Loss: 5.9312944412231445 - score: (0.59375, 0.09375, 0.16193181818181818, None) - t: 0.2001791000366211\n",
      "Loss: 5.970556735992432 - score: (0.375, 0.09375, 0.15, None) - t: 0.19982314109802246\n",
      "Loss: 5.739850997924805 - score: (0.3333333333333333, 0.125, 0.18181818181818182, None) - t: 0.19889426231384277\n",
      "Loss: 5.9696855545043945 - score: (0.1388888888888889, 0.0625, 0.08620689655172416, None) - t: 0.1993999481201172\n",
      "Loss: 6.9166460037231445 - score: (0.5208333333333333, 0.0625, 0.11160714285714285, None) - t: 0.1998450756072998\n",
      "Loss: 6.286923885345459 - score: (0.23958333333333331, 0.03125, 0.05528846153846153, None) - t: 0.20047283172607422\n",
      "Loss: 5.349720001220703 - score: (0.765625, 0.21875, 0.295255016722408, None) - t: 0.24296092987060547\n",
      "Loss: 5.566540718078613 - score: (0.4174107142857143, 0.1875, 0.20618872549019607, None) - t: 0.19888067245483398\n",
      "Loss: 6.1859965324401855 - score: (0.70625, 0.1875, 0.2849537037037037, None) - t: 0.19925403594970703\n",
      "Loss: 5.486462593078613 - score: (0.703125, 0.21875, 0.3194444444444444, None) - t: 0.20030760765075684\n",
      "Loss: 5.109547138214111 - score: (0.8250000000000001, 0.28125, 0.38510101010101006, None) - t: 0.1999950408935547\n",
      "Loss: 5.7815165519714355 - score: (0.09375, 0.09375, 0.09375, None) - t: 0.19942164421081543\n",
      "Loss: 6.118332862854004 - score: (0.08333333333333333, 0.0625, 0.07142857142857144, None) - t: 0.19933724403381348\n",
      "Loss: 6.266632556915283 - score: (0.0, 0.0, 0.0, None) - t: 0.19900894165039062\n",
      "Loss: 5.925012588500977 - score: (0.6796875, 0.21875, 0.3248626373626373, None) - t: 0.19966387748718262\n",
      "Loss: 6.350137710571289 - score: (0.59375, 0.0625, 0.11309523809523811, None) - t: 0.25559139251708984\n",
      "Loss: 6.011911392211914 - score: (0.44791666666666663, 0.125, 0.1903409090909091, None) - t: 0.19980907440185547\n",
      "Loss: 5.758936405181885 - score: (0.25, 0.09375, 0.13636363636363635, None) - t: 0.20027780532836914\n",
      "Loss: 5.196059226989746 - score: (0.390625, 0.1875, 0.2261904761904762, None) - t: 0.19991087913513184\n",
      "Loss: 6.073744773864746 - score: (0.22708333333333336, 0.09375, 0.13116776315789472, None) - t: 0.20063567161560059\n",
      "Loss: 6.497706890106201 - score: (0.3125, 0.09375, 0.13690476190476192, None) - t: 0.19899749755859375\n",
      "Loss: 5.711827278137207 - score: (0.3, 0.15625, 0.19602272727272727, None) - t: 0.19870519638061523\n",
      "Loss: 5.6083879470825195 - score: (0.22812500000000002, 0.09375, 0.13259109311740888, None) - t: 0.19919562339782715\n",
      "Loss: 5.516729831695557 - score: (0.375, 0.15625, 0.21117424242424243, None) - t: 0.1988837718963623\n",
      "Loss: 4.562848091125488 - score: (0.35625, 0.1875, 0.2423878205128205, None) - t: 0.19951343536376953\n",
      "Loss: 5.314234733581543 - score: (0.4107142857142857, 0.1875, 0.2526041666666667, None) - t: 0.199080228805542\n",
      "Loss: 6.158952236175537 - score: (0.78125, 0.15625, 0.2604166666666667, None) - t: 0.20426177978515625\n",
      "Loss: 5.796540260314941 - score: (0.71875, 0.125, 0.19625, None) - t: 0.20052289962768555\n",
      "Loss: 5.422597885131836 - score: (0.875, 0.15625, 0.24241071428571426, None) - t: 0.19841599464416504\n",
      "Loss: 4.813957691192627 - score: (0.875, 0.21875, 0.33806818181818177, None) - t: 0.2004261016845703\n",
      "Loss: 6.198787212371826 - score: (0.41666666666666663, 0.0625, 0.10869565217391305, None) - t: 0.20046782493591309\n",
      "Loss: 4.276699542999268 - score: (0.78125, 0.34375, 0.4774305555555556, None) - t: 0.19987225532531738\n",
      "Loss: 5.620710372924805 - score: (0.75, 0.1875, 0.30000000000000004, None) - t: 0.19947314262390137\n",
      "Loss: 6.674121856689453 - score: (0.5625, 0.0625, 0.11249999999999999, None) - t: 0.1992354393005371\n",
      "Loss: 5.740261077880859 - score: (0.53125, 0.1875, 0.27717391304347827, None) - t: 0.2008044719696045\n",
      "Loss: 5.0263848304748535 - score: (0.421875, 0.25, 0.30520833333333336, None) - t: 0.2002396583557129\n",
      "Loss: 6.185855865478516 - score: (0.40625, 0.15625, 0.22499999999999998, None) - t: 0.21053194999694824\n",
      "Loss: 5.829582214355469 - score: (0.03125, 0.03125, 0.03125, None) - t: 0.20001673698425293\n",
      "Loss: 5.5748090744018555 - score: (0.34375, 0.15625, 0.21484375, None) - t: 0.19802045822143555\n",
      "Loss: 5.118459701538086 - score: (0.35546875, 0.21875, 0.27083333333333337, None) - t: 0.20139026641845703\n",
      "Loss: 5.874114990234375 - score: (0.40625, 0.15625, 0.22569444444444445, None) - t: 0.1992778778076172\n",
      "Loss: 6.0152082443237305 - score: (0.3125, 0.0625, 0.10416666666666669, None) - t: 0.1987755298614502\n",
      "Loss: 5.449032783508301 - score: (0.40625, 0.125, 0.19117647058823528, None) - t: 0.1997222900390625\n",
      "Loss: 5.5681986808776855 - score: (0.3125, 0.125, 0.171875, None) - t: 0.2249758243560791\n",
      "Loss: 5.591134071350098 - score: (0.40625, 0.15625, 0.21714743589743588, None) - t: 0.19870972633361816\n",
      "Loss: 5.2580647468566895 - score: (0.24609375, 0.21875, 0.23161764705882354, None) - t: 0.19899749755859375\n",
      "Loss: 5.599246501922607 - score: (0.28125, 0.125, 0.17307692307692304, None) - t: 0.20023655891418457\n",
      "Loss: 5.41218376159668 - score: (0.21875, 0.0625, 0.09375, None) - t: 0.19972848892211914\n",
      "Loss: 4.577356815338135 - score: (0.3375, 0.21875, 0.246875, None) - t: 0.19980239868164062\n",
      "Loss: 6.078370571136475 - score: (0.34375, 0.125, 0.1753472222222222, None) - t: 0.20026087760925293\n",
      "Loss: 5.727752685546875 - score: (0.1875, 0.09375, 0.125, None) - t: 0.19914603233337402\n",
      "Loss: 4.383258819580078 - score: (0.24151785714285712, 0.1875, 0.2026785714285714, None) - t: 0.19972920417785645\n",
      "Loss: 5.620371341705322 - score: (0.265625, 0.125, 0.16319444444444445, None) - t: 0.20053815841674805\n",
      "Loss: 5.607313632965088 - score: (0.4375, 0.1875, 0.24107142857142858, None) - t: 0.19957733154296875\n",
      "Loss: 6.689322471618652 - score: (0.0625, 0.03125, 0.041666666666666664, None) - t: 0.20016241073608398\n",
      "Loss: 5.289402484893799 - score: (0.45, 0.15625, 0.23055555555555557, None) - t: 0.22956395149230957\n",
      "Loss: 5.944416046142578 - score: (0.40625, 0.125, 0.18898809523809523, None) - t: 0.20057010650634766\n",
      "Loss: 7.04169225692749 - score: (0.375, 0.09375, 0.11931818181818182, None) - t: 0.2004871368408203\n",
      "Loss: 5.185772895812988 - score: (0.3020833333333333, 0.125, 0.165625, None) - t: 0.20022916793823242\n",
      "Loss: 5.33056116104126 - score: (0.3515625, 0.125, 0.18437499999999998, None) - t: 0.21460270881652832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.263310432434082 - score: (0.328125, 0.1875, 0.22499999999999998, None) - t: 0.21109247207641602\n",
      "Loss: 5.795498371124268 - score: (0.15625, 0.09375, 0.11458333333333333, None) - t: 0.1995549201965332\n",
      "Loss: 6.6850972175598145 - score: (0.28125, 0.09375, 0.13125, None) - t: 0.20050811767578125\n",
      "Loss: 6.179559707641602 - score: (0.03125, 0.03125, 0.03125, None) - t: 0.20031213760375977\n",
      "Loss: 5.755373001098633 - score: (0.46875, 0.125, 0.19736842105263158, None) - t: 0.19981622695922852\n",
      "Loss: 5.314488887786865 - score: (0.5, 0.09375, 0.15520833333333334, None) - t: 0.19871997833251953\n",
      "Loss: 5.669483184814453 - score: (0.484375, 0.28125, 0.34374999999999994, None) - t: 0.1999819278717041\n",
      "Loss: 5.829525470733643 - score: (0.2708333333333333, 0.0625, 0.1015625, None) - t: 0.24324822425842285\n",
      "Loss: 3.9770607948303223 - score: (0.37053571428571425, 0.28125, 0.3189338235294118, None) - t: 0.2438960075378418\n",
      "Loss: 4.803102016448975 - score: (0.140625, 0.09375, 0.10714285714285715, None) - t: 0.2562432289123535\n",
      "Loss: 5.781642913818359 - score: (0.1375, 0.09375, 0.10984848484848483, None) - t: 0.22372055053710938\n",
      "Loss: 6.0730509757995605 - score: (0.41666666666666663, 0.125, 0.184862012987013, None) - t: 0.23650574684143066\n",
      "Loss: 5.963908672332764 - score: (0.421875, 0.09375, 0.13636363636363635, None) - t: 0.19922184944152832\n",
      "Loss: 5.138931751251221 - score: (0.3645833333333333, 0.1875, 0.24739583333333334, None) - t: 0.20022869110107422\n",
      "Loss: 5.592021942138672 - score: (0.40625, 0.09375, 0.15234375, None) - t: 0.20094728469848633\n",
      "Loss: 6.204535961151123 - score: (0.2890625, 0.125, 0.16874999999999998, None) - t: 0.20616960525512695\n",
      "Loss: 6.0468573570251465 - score: (0.29166666666666663, 0.0625, 0.10294117647058823, None) - t: 0.2275705337524414\n",
      "Loss: 5.770027160644531 - score: (0.34375, 0.09375, 0.11875, None) - t: 0.20049524307250977\n",
      "Loss: 5.894845962524414 - score: (0.34375, 0.09375, 0.14732142857142858, None) - t: 0.22343707084655762\n",
      "Loss: 5.0453782081604 - score: (0.53125, 0.1875, 0.2673611111111111, None) - t: 0.19985532760620117\n",
      "Loss: 5.324550151824951 - score: (0.3984375, 0.125, 0.18910256410256407, None) - t: 0.19884300231933594\n",
      "Loss: 4.94984245300293 - score: (0.3671875, 0.09375, 0.14393939393939392, None) - t: 0.19928526878356934\n",
      "Loss: 5.268283843994141 - score: (0.79375, 0.125, 0.1852678571428571, None) - t: 0.19971966743469238\n",
      "Loss: 5.673912048339844 - score: (0.5572916666666666, 0.09375, 0.13290229885057472, None) - t: 0.19927382469177246\n",
      "Loss: 5.460424900054932 - score: (0.9453125, 0.25, 0.3629901960784314, None) - t: 0.2009439468383789\n",
      "Loss: 5.688671112060547 - score: (0.90625, 0.1875, 0.3107142857142857, None) - t: 0.19898653030395508\n",
      "Loss: 5.988428592681885 - score: (0.9375, 0.125, 0.22058823529411764, None) - t: 0.1994946002960205\n",
      "Loss: 4.717710494995117 - score: (0.875, 0.15625, 0.26515151515151514, None) - t: 0.20005297660827637\n",
      "Loss: 5.148554801940918 - score: (0.90625, 0.09375, 0.169921875, None) - t: 0.1994307041168213\n",
      "Loss: 6.006913661956787 - score: (0.90625, 0.0625, 0.11693548387096774, None) - t: 0.20070242881774902\n",
      "Loss: 5.072126388549805 - score: (0.90625, 0.15625, 0.25, None) - t: 0.20072031021118164\n",
      "Loss: 5.03514289855957 - score: (0.90625, 0.125, 0.21969696969696967, None) - t: 0.20036911964416504\n",
      "Loss: 5.43008279800415 - score: (0.7265625, 0.125, 0.17836538461538465, None) - t: 0.2002274990081787\n",
      "Loss: 5.184756755828857 - score: (0.3828125, 0.1875, 0.23913043478260873, None) - t: 0.2008967399597168\n",
      "Loss: 6.057271957397461 - score: (0.11458333333333333, 0.0625, 0.07670454545454546, None) - t: 0.20015668869018555\n",
      "Loss: 5.832194805145264 - score: (0.59375, 0.09375, 0.16193181818181818, None) - t: 0.20009970664978027\n",
      "Loss: 5.809160232543945 - score: (0.625, 0.1875, 0.27864583333333337, None) - t: 0.20055055618286133\n",
      "Loss: 5.461705684661865 - score: (0.625, 0.15625, 0.23777173913043478, None) - t: 0.20047926902770996\n",
      "Loss: 5.576408386230469 - score: (0.46875, 0.09375, 0.140625, None) - t: 0.19991850852966309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/25 [00:58<23:27, 58.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.994266986846924 - score: (0.65625, 0.15625, 0.24818840579710144, None) - t: 0.1993546485900879\n",
      "Loss: 6.107772350311279 - score: (0.5, 0.1, 0.16666666666666669, None) - t: 0.06648540496826172\n",
      "Loss: 5.90203332901001 - score: (0.0, 0.0, 0.0, None) - t: 0.19936466217041016\n",
      "Loss: 4.563817501068115 - score: (0.42857142857142855, 0.1875, 0.26086956521739124, None) - t: 0.20107436180114746\n",
      "Loss: 5.312638282775879 - score: (0.40625, 0.03125, 0.058035714285714295, None) - t: 0.20245051383972168\n",
      "Loss: 6.0045881271362305 - score: (0.9375, 0.0625, 0.1171875, None) - t: 0.20288515090942383\n",
      "Loss: 5.186038970947266 - score: (0.9375, 0.0625, 0.1171875, None) - t: 0.20037603378295898\n",
      "Loss: 5.112027645111084 - score: (0.96875, 0.15625, 0.26909722222222215, None) - t: 0.21403098106384277\n",
      "Loss: 5.427913665771484 - score: (0.96875, 0.0625, 0.11742424242424243, None) - t: 0.22737836837768555\n",
      "Loss: 5.606229782104492 - score: (0.96875, 0.125, 0.22142857142857145, None) - t: 0.2007899284362793\n",
      "Loss: 5.623204708099365 - score: (1.0, 0.1875, 0.31575520833333337, None) - t: 0.19878101348876953\n",
      "Loss: 4.530680179595947 - score: (0.90625, 0.0625, 0.11693548387096774, None) - t: 0.2395005226135254\n",
      "Loss: 4.973324298858643 - score: (0.6875, 0.125, 0.21153846153846156, None) - t: 0.22220945358276367\n",
      "Loss: 5.424716949462891 - score: (0.5562499999999999, 0.125, 0.2014266304347826, None) - t: 0.22811579704284668\n",
      "Loss: 6.1798810958862305 - score: (0.6875, 0.0625, 0.11458333333333334, None) - t: 0.2024393081665039\n",
      "Loss: 6.278347015380859 - score: (0.5625, 0.0625, 0.11249999999999999, None) - t: 0.22815203666687012\n",
      "Loss: 5.202906131744385 - score: (0.625, 0.125, 0.20833333333333337, None) - t: 0.2283155918121338\n",
      "Loss: 4.405068874359131 - score: (0.5625, 0.125, 0.20454545454545453, None) - t: 0.2097175121307373\n",
      "Loss: 5.0367536544799805 - score: (0.78125, 0.125, 0.2155172413793104, None) - t: 0.22419095039367676\n",
      "Loss: 5.799863338470459 - score: (0.625, 0.0625, 0.11363636363636365, None) - t: 0.2250199317932129\n",
      "Loss: 4.534387111663818 - score: (0.359375, 0.03125, 0.0575, None) - t: 0.21004819869995117\n",
      "Loss: 5.497432708740234 - score: (0.71875, 0.09375, 0.16586538461538464, None) - t: 0.219804048538208\n",
      "Loss: 4.646676540374756 - score: (0.71875, 0.125, 0.18863636363636366, None) - t: 0.22344517707824707\n",
      "Loss: 5.789218902587891 - score: (0.1875, 0.09375, 0.125, None) - t: 0.2318572998046875\n",
      "Loss: 5.307431697845459 - score: (0.78125, 0.15625, 0.2517361111111111, None) - t: 0.22509336471557617\n",
      "Loss: 4.952198028564453 - score: (0.75, 0.21875, 0.31931818181818183, None) - t: 0.21190381050109863\n",
      "Loss: 5.212561130523682 - score: (0.21875, 0.0625, 0.09722222222222222, None) - t: 0.22120404243469238\n",
      "Loss: 5.233717441558838 - score: (0.625, 0.1875, 0.26515151515151514, None) - t: 0.22595429420471191\n",
      "Loss: 4.703099250793457 - score: (0.96875, 0.1875, 0.3127604166666667, None) - t: 0.22435760498046875\n",
      "Loss: 5.198512554168701 - score: (0.71875, 0.21875, 0.3003393665158371, None) - t: 0.22674012184143066\n",
      "Loss: 5.371372222900391 - score: (0.40625, 0.125, 0.19117647058823528, None) - t: 0.22356295585632324\n",
      "Loss: 5.01657247543335 - score: (0.4375, 0.125, 0.19444444444444445, None) - t: 0.19988369941711426\n",
      "Loss: 5.255943298339844 - score: (0.5, 0.09375, 0.15696022727272727, None) - t: 0.22694158554077148\n",
      "Loss: 5.968466758728027 - score: (0.34375, 0.09375, 0.14732142857142858, None) - t: 0.21681499481201172\n",
      "Loss: 5.826845645904541 - score: (0.75, 0.09375, 0.16380494505494508, None) - t: 0.19893360137939453\n",
      "Loss: 4.425290584564209 - score: (0.40625, 0.15625, 0.22569444444444445, None) - t: 0.22743463516235352\n",
      "Loss: 5.7620439529418945 - score: (0.4375, 0.15625, 0.2285714285714286, None) - t: 0.19891142845153809\n",
      "Loss: 5.382954120635986 - score: (0.8125, 0.0625, 0.11588541666666667, None) - t: 0.20196151733398438\n",
      "Loss: 4.521594047546387 - score: (0.5, 0.1875, 0.2727272727272727, None) - t: 0.2002706527709961\n",
      "Loss: 5.84663200378418 - score: (0.109375, 0.0625, 0.07589285714285715, None) - t: 0.2086043357849121\n",
      "Loss: 5.11260986328125 - score: (0.53125, 0.125, 0.19791666666666669, None) - t: 0.22389554977416992\n",
      "Loss: 4.729621887207031 - score: (0.5, 0.28125, 0.3596491228070175, None) - t: 0.23154067993164062\n",
      "Loss: 5.160317420959473 - score: (0.0, 0.0, 0.0, None) - t: 0.21066021919250488\n",
      "Loss: 5.778024673461914 - score: (0.3125, 0.09375, 0.14423076923076922, None) - t: 0.20616436004638672\n",
      "Loss: 4.997598171234131 - score: (0.24107142857142855, 0.1875, 0.2109375, None) - t: 0.22512006759643555\n",
      "Loss: 5.409868240356445 - score: (0.46875, 0.21875, 0.29264705882352937, None) - t: 0.2302553653717041\n",
      "Loss: 5.454529762268066 - score: (0.4375, 0.15625, 0.23026315789473684, None) - t: 0.22809624671936035\n",
      "Loss: 4.921438217163086 - score: (0.5625, 0.09375, 0.1607142857142857, None) - t: 0.2212672233581543\n",
      "Loss: 4.901937961578369 - score: (0.5, 0.03125, 0.058823529411764705, None) - t: 0.19905424118041992\n",
      "Loss: 5.4090962409973145 - score: (0.5625, 0.125, 0.20454545454545453, None) - t: 0.20071029663085938\n",
      "Loss: 5.109271049499512 - score: (0.390625, 0.15625, 0.2232142857142857, None) - t: 0.20075297355651855\n",
      "Loss: 6.11467981338501 - score: (0.375, 0.0625, 0.10714285714285714, None) - t: 0.2249927520751953\n",
      "Loss: 4.842586994171143 - score: (0.4375, 0.09375, 0.15441176470588236, None) - t: 0.20015358924865723\n",
      "Loss: 4.707059860229492 - score: (0.546875, 0.15625, 0.21726190476190477, None) - t: 0.20456552505493164\n",
      "Loss: 5.721079349517822 - score: (0.5, 0.15625, 0.23809523809523808, None) - t: 0.203902006149292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6800f6e32bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                      \u001b[0mtraining\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                      \u001b[0mtotal_loss_over_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss_over_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                      scores_over_epochs     = scores_over_epochs)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     train_scores = metrics.precision_recall_fscore_support(tr_labels,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-460ef9dc0c0c>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(data_loader, model, criterion, optimizer, batch_size, training, total_loss_over_epochs, scores_over_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         loss = criterion(logits, a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         score = metrics.precision_recall_fscore_support(preds.tolist(),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = dataset.build_data_loader(train=True, args={'batch_size': batch_size})\n",
    "test_loader  = dataset.build_data_loader(test=True, args={'batch_size': batch_size})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "train_all_loss, train_all_labels, train_all_preds = [], [], []\n",
    "\n",
    "total_loss_over_epochs, scores_over_epochs = plotting.get_empty_stat_over_n_epoch_dictionaries()\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    t0= time.time()\n",
    "    tr_loss, tr_labels, tr_preds = eval_model(data_loader = train_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = optimizer,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = True,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs,\n",
    "                                     scores_over_epochs     = scores_over_epochs)\n",
    "    \n",
    "#     train_scores = metrics.precision_recall_fscore_support(tr_labels,\n",
    "#                                                            tr_preds,\n",
    "#                                                            average='weighted')\n",
    "    \n",
    "#     total_loss_over_epochs['train_loss'].append(tr_loss)\n",
    "#     scores_over_epochs['train_scores'].append(train_scores)\n",
    "    \n",
    "#     if True:# or epoch%1 == 0:\n",
    "#         print(\"#==#\"*5 + \"epoch: {}\".format(epoch) + \"#==#\"*5)\n",
    "#         print(\"time: {}\".format(time.time()-t0))\n",
    "#         print(train_scores)\n",
    "#     plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(8,5))\n",
    "#     plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(8, 5), title=\"Loss\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tr_labels))\n",
    "print(type(tr_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_labels[0])\n",
    "print(tr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
