{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import plotting\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import imread, img_data_2_mini_batch, imgs2batch\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from naive import Enc, Dec, EncDec\n",
    "from data_loader import VQADataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data_2000.pkl\n",
      "reading from ./data/data_2000.pkl\n"
     ]
    }
   ],
   "source": [
    "N = 2000\n",
    "dataset_filename = \"./data/data_{}.pkl\".format(N)\n",
    "dataset = None\n",
    "print(dataset_filename)\n",
    "if (os.path.exists(dataset_filename)):\n",
    "    with open(dataset_filename, 'rb') as handle:\n",
    "        print(\"reading from \" + dataset_filename)\n",
    "        dataset = pickle.load(handle)\n",
    "else:\n",
    "    dataset = VQADataSet(Q=N)\n",
    "    with open(dataset_filename, 'wb') as handle:\n",
    "        print(\"writing to \" + dataset_filename)\n",
    "        pickle.dump(dataset, handle)\n",
    "\n",
    "assert(dataset is not None)\n",
    "def debug(v,q,a):\n",
    "    print('\\nV: {}\\nQ: {}\\nA: {}'.format(v.shape, q.shape, a.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size        = 128\n",
    "hidden_size       = 128\n",
    "batch_size        = 32\n",
    "ques_vocab_size   = len(dataset.vocab['question'])\n",
    "ans_vocab_size    = len(dataset.vocab['answer'])\n",
    "rnn_layers        = 1\n",
    "n_epochs          = 50\n",
    "learning_rate     = 0.001\n",
    "momentum          = 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, criterion, optimizer, batch_size, training=False,\n",
    "              total_loss_over_epochs=[], scores_over_epochs=[]):\n",
    "    running_loss = 0.\n",
    "    final_labels, final_preds = [], []\n",
    "    if data_loader is None:\n",
    "        return\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    for i, minibatch in enumerate(data_loader):\n",
    "        # extract minibatch\n",
    "        t0 = time.time()\n",
    "        idxs, v, q, a, q_len = minibatch\n",
    "        \n",
    "        # convert torch's DataLoader output to proper format.\n",
    "        # torch gives a List[Tensor_1, ... ] where tensor has been transposed. \n",
    "        # batchify transposes back.`\n",
    "        v = v.to(device)\n",
    "        q = VQADataSet.batchify_questions(q).to(device)\n",
    "        a = a.to(device)\n",
    "\n",
    "        logits = model(v, q, q_len)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        loss = criterion(logits, a)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        score = metrics.precision_recall_fscore_support(preds.tolist(),\n",
    "                                                        a.tolist(),\n",
    "                                                        average='weighted')\n",
    "        \n",
    "        total_loss_over_epochs['train_loss'].append(loss)\n",
    "        scores_over_epochs['train_scores'].append(score)\n",
    "        \n",
    "        if training and optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        final_labels += a.tolist()\n",
    "        final_preds  += preds.tolist()\n",
    "        if True:#(i%20==0):\n",
    "#             plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(7,3))\n",
    "#             plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(7, 3), title=\"Loss\")\n",
    "            print(\"Loss: {} - score: {} - t: {}\".format(loss, score, time.time()-t0))\n",
    "            \n",
    "    return running_loss, final_labels, final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EncDec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, rnn_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32 shuffle: True\n",
      "batch_size: 32 shuffle: False\n",
      "Loss: 7.166294097900391 - score: (0.0, 0.0, 0.0, None) - t: 0.26448512077331543\n",
      "Loss: 7.170238018035889 - score: (0.0, 0.0, 0.0, None) - t: 0.24040842056274414\n",
      "Loss: 7.1570210456848145 - score: (0.0, 0.0, 0.0, None) - t: 0.24123334884643555\n",
      "Loss: 7.142458915710449 - score: (0.0, 0.0, 0.0, None) - t: 0.2391650676727295\n",
      "Loss: 7.176437854766846 - score: (0.0, 0.0, 0.0, None) - t: 0.24075031280517578\n",
      "Loss: 7.152554035186768 - score: (0.0, 0.0, 0.0, None) - t: 0.24045491218566895\n",
      "Loss: 7.151719570159912 - score: (0.0, 0.0, 0.0, None) - t: 0.24098420143127441\n",
      "Loss: 7.134801864624023 - score: (0.0, 0.0, 0.0, None) - t: 0.24095916748046875\n",
      "Loss: 7.153505325317383 - score: (0.0, 0.0, 0.0, None) - t: 0.24059104919433594\n",
      "Loss: 7.128856658935547 - score: (0.0, 0.0, 0.0, None) - t: 0.2414393424987793\n",
      "Loss: 7.133441925048828 - score: (0.0, 0.0, 0.0, None) - t: 0.2401435375213623\n",
      "Loss: 7.128518581390381 - score: (0.0, 0.0, 0.0, None) - t: 0.24097800254821777\n",
      "Loss: 7.1446852684021 - score: (0.0, 0.0, 0.0, None) - t: 0.24079275131225586\n",
      "Loss: 7.16806173324585 - score: (0.0, 0.0, 0.0, None) - t: 0.2400360107421875\n",
      "Loss: 7.171440124511719 - score: (0.0, 0.0, 0.0, None) - t: 0.24020838737487793\n",
      "Loss: 7.098502159118652 - score: (0.0, 0.0, 0.0, None) - t: 0.24132418632507324\n",
      "Loss: 7.149331092834473 - score: (0.0, 0.0, 0.0, None) - t: 0.23969769477844238\n",
      "Loss: 7.156115531921387 - score: (0.0, 0.0, 0.0, None) - t: 0.24047183990478516\n",
      "Loss: 7.109039306640625 - score: (0.0, 0.0, 0.0, None) - t: 0.2414848804473877\n",
      "Loss: 7.135012149810791 - score: (0.005208333333333333, 0.03125, 0.008928571428571428, None) - t: 0.24130535125732422\n",
      "Loss: 7.120340347290039 - score: (0.0, 0.0, 0.0, None) - t: 0.24361228942871094\n",
      "Loss: 7.11250114440918 - score: (0.0, 0.0, 0.0, None) - t: 0.2392258644104004\n",
      "Loss: 7.1164445877075195 - score: (0.1875, 0.03125, 0.05357142857142857, None) - t: 0.24091672897338867\n",
      "Loss: 7.118879795074463 - score: (0.0, 0.0, 0.0, None) - t: 0.24019432067871094\n",
      "Loss: 7.081172943115234 - score: (0.041666666666666664, 0.03125, 0.03571428571428572, None) - t: 0.24066853523254395\n",
      "Loss: 7.121526718139648 - score: (0.0, 0.0, 0.0, None) - t: 0.24175429344177246\n",
      "Loss: 7.118992328643799 - score: (0.09375, 0.03125, 0.046875, None) - t: 0.24124956130981445\n",
      "Loss: 7.104936122894287 - score: (0.0, 0.0, 0.0, None) - t: 0.23990559577941895\n",
      "Loss: 7.062983512878418 - score: (0.28125, 0.09375, 0.140625, None) - t: 0.24076223373413086\n",
      "Loss: 7.115560054779053 - score: (0.46875, 0.03125, 0.05859375, None) - t: 0.24185895919799805\n",
      "Loss: 7.080036640167236 - score: (0.19375, 0.0625, 0.06398809523809525, None) - t: 0.24031543731689453\n",
      "Loss: 7.131628036499023 - score: (0.5, 0.0625, 0.1111111111111111, None) - t: 0.24085593223571777\n",
      "Loss: 7.05819845199585 - score: (0.46875, 0.09375, 0.15624999999999997, None) - t: 0.2394397258758545\n",
      "Loss: 7.0689473152160645 - score: (0.625, 0.125, 0.20833333333333337, None) - t: 0.24116110801696777\n",
      "Loss: 7.029738426208496 - score: (0.3125, 0.0625, 0.10416666666666666, None) - t: 0.2415471076965332\n",
      "Loss: 7.064006805419922 - score: (0.3958333333333333, 0.125, 0.19, None) - t: 0.24083995819091797\n",
      "Loss: 7.098954200744629 - score: (0.4140625, 0.09375, 0.12083333333333335, None) - t: 0.24058818817138672\n",
      "Loss: 7.097854137420654 - score: (0.0, 0.0, 0.0, None) - t: 0.24138784408569336\n",
      "Loss: 7.070912837982178 - score: (0.65625, 0.0625, 0.11413043478260868, None) - t: 0.24120259284973145\n",
      "Loss: 7.088192462921143 - score: (0.265625, 0.0625, 0.10119047619047619, None) - t: 0.24330353736877441\n",
      "Loss: 7.132691383361816 - score: (0.3333333333333333, 0.0625, 0.10526315789473684, None) - t: 0.2418830394744873\n",
      "Loss: 7.072182655334473 - score: (0.328125, 0.09375, 0.14583333333333331, None) - t: 0.24011826515197754\n",
      "Loss: 7.016898155212402 - score: (0.6015625, 0.15625, 0.24737394957983191, None) - t: 0.24053049087524414\n",
      "Loss: 7.0678277015686035 - score: (0.53125, 0.15625, 0.2414772727272727, None) - t: 0.24088716506958008\n",
      "Loss: 7.070826053619385 - score: (0.5, 0.09375, 0.15789473684210525, None) - t: 0.2414846420288086\n",
      "Loss: 7.007981777191162 - score: (0.4453125, 0.09375, 0.15476190476190477, None) - t: 0.24030852317810059\n",
      "Loss: 7.0220136642456055 - score: (0.3958333333333333, 0.125, 0.19, None) - t: 0.24088835716247559\n",
      "Loss: 7.025505065917969 - score: (0.3515625, 0.09375, 0.1480263157894737, None) - t: 0.2408287525177002\n",
      "Loss: 7.013746738433838 - score: (0.53125, 0.15625, 0.2414772727272727, None) - t: 0.24200677871704102\n",
      "Loss: 7.033876895904541 - score: (0.4375, 0.0625, 0.109375, None) - t: 0.24158334732055664\n",
      "Loss: 6.964335918426514 - score: (0.15, 0.09375, 0.11538461538461538, None) - t: 0.24074029922485352\n",
      "Loss: 7.073452949523926 - score: (1.0, 0.09375, 0.16858552631578946, None) - t: 0.24248695373535156\n",
      "Loss: 6.8803629875183105 - score: (0.3515625, 0.15625, 0.21634615384615383, None) - t: 0.24244260787963867\n",
      "Loss: 7.083333492279053 - score: (0.3125, 0.03125, 0.05681818181818182, None) - t: 0.2427995204925537\n",
      "Loss: 7.068748474121094 - score: (0.08333333333333333, 0.03125, 0.045454545454545456, None) - t: 0.24147796630859375\n",
      "Loss: 6.946025848388672 - score: (0.5833333333333334, 0.21875, 0.3181818181818182, None) - t: 0.24075698852539062\n",
      "Loss: 7.017247200012207 - score: (0.16874999999999998, 0.09375, 0.12053571428571427, None) - t: 0.24206161499023438\n",
      "Loss: 6.95128870010376 - score: (0.621875, 0.15625, 0.2371031746031746, None) - t: 0.2418978214263916\n",
      "Loss: 6.991644859313965 - score: (0.340625, 0.0625, 0.10344251336898394, None) - t: 0.24202251434326172\n",
      "Loss: 6.961401462554932 - score: (0.5078125, 0.125, 0.19982142857142857, None) - t: 0.24237656593322754\n",
      "Loss: 7.016317844390869 - score: (0.2708333333333333, 0.0625, 0.1015625, None) - t: 0.24273300170898438\n",
      "Loss: 6.948361873626709 - score: (0.46875, 0.09375, 0.15625000000000003, None) - t: 0.24200177192687988\n",
      "Loss: 6.981532096862793 - score: (0.0, 0.0, 0.0, None) - t: 0.24090981483459473\n",
      "Loss: 6.939544677734375 - score: (0.5833333333333333, 0.125, 0.19333333333333333, None) - t: 0.2410118579864502\n",
      "Loss: 6.919721603393555 - score: (0.3515625, 0.1875, 0.24456521739130438, None) - t: 0.24178028106689453\n",
      "Loss: 6.974616527557373 - score: (0.59375, 0.125, 0.20436507936507936, None) - t: 0.24233579635620117\n",
      "Loss: 7.075333595275879 - score: (0.9375, 0.0625, 0.09166666666666667, None) - t: 0.2434985637664795\n",
      "Loss: 6.923300743103027 - score: (0.6175595238095237, 0.09375, 0.13203125, None) - t: 0.242476224899292\n",
      "Loss: 6.990057945251465 - score: (0.96875, 0.0625, 0.11742424242424243, None) - t: 0.24179291725158691\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b3da5a8ac802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                      \u001b[0mtraining\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                      \u001b[0mtotal_loss_over_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss_over_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                      scores_over_epochs     = scores_over_epochs)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     train_scores = metrics.precision_recall_fscore_support(tr_labels,\n",
      "\u001b[0;32m<ipython-input-9-8897f8dfe7f8>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(data_loader, model, criterion, optimizer, batch_size, training, total_loss_over_epochs, scores_over_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# extract minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Visual-Question-Answering/data_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mquestion_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mv\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0mq\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mquestion_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0ma\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0manswer_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Visual-Question-Answering/utils.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(path, transform)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IMG_LOAD_ERR - Image File idx={}: [{}] not found\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = dataset.build_data_loader(train=True, args={'batch_size': batch_size})\n",
    "test_loader  = dataset.build_data_loader(test=True, args={'batch_size': batch_size})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "train_all_loss, train_all_labels, train_all_preds = [], [], []\n",
    "\n",
    "total_loss_over_epochs, scores_over_epochs = plotting.get_empty_stat_over_n_epoch_dictionaries()\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    t0= time.time()\n",
    "    tr_loss, tr_labels, tr_preds = eval_model(data_loader = train_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = optimizer,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = True,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs,\n",
    "                                     scores_over_epochs     = scores_over_epochs)\n",
    "    \n",
    "    train_scores = metrics.precision_recall_fscore_support(tr_labels,\n",
    "                                                           tr_preds,\n",
    "                                                           average='weighted')\n",
    "    \n",
    "    total_loss_over_epochs['train_loss'].append(tr_loss)\n",
    "    scores_over_epochs['train_scores'].append(train_scores)\n",
    "    \n",
    "#     if True:# or epoch%1 == 0:\n",
    "#         print(\"#==#\"*5 + \"epoch: {}\".format(epoch) + \"#==#\"*5)\n",
    "#         print(\"time: {}\".format(time.time()-t0))\n",
    "#         print(train_scores)\n",
    "#     plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(8,5))\n",
    "#     plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(8, 5), title=\"Loss\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tr_labels))\n",
    "print(type(tr_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_labels[0])\n",
    "print(tr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
