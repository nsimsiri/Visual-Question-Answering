{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import plotting\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import imread, img_data_2_mini_batch, imgs2batch\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from naive import EncDec\n",
    "# from attention import EncDec as FuseAttEncDec\n",
    "# from rnn_att import EncDec\n",
    "from data_loader import VQADataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data_5000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:13<00:00, 377.07it/s]\n",
      "100%|██████████| 4937/4937 [00:00<00:00, 548944.35it/s]\n",
      "100%|██████████| 4937/4937 [00:00<00:00, 5276.82it/s]\n",
      "100%|██████████| 4937/4937 [00:00<00:00, 466390.66it/s]\n",
      "100%|██████████| 4937/4937 [00:00<00:00, 233184.83it/s]\n",
      "100%|██████████| 1022/1022 [00:00<00:00, 41637.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQADataSet init time: 19.127565622329712\n",
      "writing to ./data/data_5000.pkl\n"
     ]
    }
   ],
   "source": [
    "N = 5000\n",
    "dataset_filename = \"./data/data_{}.pkl\".format(N)\n",
    "dataset = None\n",
    "print(dataset_filename)\n",
    "if (os.path.exists(dataset_filename)):\n",
    "    with open(dataset_filename, 'rb') as handle:\n",
    "        print(\"reading from \" + dataset_filename)\n",
    "        dataset = pickle.load(handle)\n",
    "else:\n",
    "    dataset = VQADataSet(Q=N)\n",
    "    with open(dataset_filename, 'wb') as handle:\n",
    "        print(\"writing to \" + dataset_filename)\n",
    "        pickle.dump(dataset, handle)\n",
    "\n",
    "assert(dataset is not None)\n",
    "def debug(v,q,a):\n",
    "    print('\\nV: {}\\nQ: {}\\nA: {}'.format(v.shape, q.shape, a.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386 1022\n"
     ]
    }
   ],
   "source": [
    "embed_size        = 128\n",
    "hidden_size       = 128\n",
    "batch_size        = 50\n",
    "ques_vocab_size   = len(dataset.vocab['question'])\n",
    "ans_vocab_size    = len(dataset.vocab['answer'])\n",
    "num_layers        = 1\n",
    "n_epochs          = 10\n",
    "learning_rate     = 0.001\n",
    "momentum          = 0.98\n",
    "attention_size    = 512\n",
    "debug             = False\n",
    "\n",
    "print(ques_vocab_size, ans_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, criterion, optimizer, batch_size, training=False,\n",
    "              epoch = 0, total_loss_over_epochs=[], scores_over_epochs=[]):\n",
    "    running_loss = 0.\n",
    "    final_labels, final_preds = [], []\n",
    "    scores, losses = [], []\n",
    "    if data_loader is None:\n",
    "        return\n",
    "    \n",
    "    run_type = None\n",
    "    if training:\n",
    "        run_type = 'train'\n",
    "        model.train()\n",
    "    else:\n",
    "        run_type = 'test'\n",
    "        model.eval()\n",
    "    \n",
    "    for i, minibatch in enumerate(data_loader):\n",
    "        # extract minibatch\n",
    "        t0 = time.time()\n",
    "        idxs, v, q, a, q_len = minibatch\n",
    "        \n",
    "        # convert torch's DataLoader output to proper format.\n",
    "        # torch gives a List[Tensor_1, ... ] where tensor has been transposed. \n",
    "        # batchify transposes back.`\n",
    "        v = v.to(device)\n",
    "        q = VQADataSet.batchify_questions(q).to(device)\n",
    "        a = a.to(device)\n",
    "\n",
    "        logits = model(v, q, q_len)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "#         loss = criterion(logits, a)\n",
    "        loss = F.nll_loss(logits, a)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "#         score = metrics.precision_recall_fscore_support(preds.tolist(),\n",
    "#                                                         a.tolist(),\n",
    "#                                                         average='weighted')\n",
    "        score = metrics.accuracy_score(preds.tolist(),a.tolist())\n",
    "    \n",
    "        scores.append(score)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        loss_key = '{}_loss'.format(run_type)\n",
    "        total_loss_over_epochs['{}_loss'.format(run_type)].append(loss)\n",
    "        scores_over_epochs['{}_scores'.format(run_type)].append(score)\n",
    "        \n",
    "        if training and optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "   \n",
    "        final_labels += a.tolist()\n",
    "        final_preds  += preds.tolist()\n",
    "        if i%10==0:\n",
    "            score = np.mean(scores)\n",
    "            print(\"Epoch {}: {} Loss: {} Score: {} t: {}\".format(epoch, run_type,loss, score, time.time()-t0))\n",
    "#             plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(7,3))\n",
    "#             plotting.plot_loss_over_n_epochs(total_loss_over_epochs, hard_key=loss_key, fig_size=(7, 3))\n",
    "            \n",
    "    return running_loss, final_labels, final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 50 shuffle: True\n",
      "batch_size: 50 shuffle: False\n",
      "model built, start training.\n",
      "Epoch 0: train Loss: 6.98696231842041 Score: 0.0 t: 0.36098647117614746\n",
      "Epoch 0: train Loss: 5.743535995483398 Score: 0.12000000000000001 t: 0.3332839012145996\n",
      "Epoch 0: train Loss: 4.490931510925293 Score: 0.13904761904761906 t: 0.3412759304046631\n",
      "Epoch 0: train Loss: 5.173640727996826 Score: 0.15548387096774194 t: 0.334744930267334\n",
      "Epoch 0: train Loss: 4.484323978424072 Score: 0.15658536585365854 t: 0.33466053009033203\n",
      "Epoch 0: train Loss: 5.701234817504883 Score: 0.15803921568627452 t: 0.3457608222961426\n",
      "Epoch 0: train Loss: 4.79588508605957 Score: 0.16098360655737703 t: 0.33469414710998535\n",
      "Epoch 0: train Loss: 5.130486965179443 Score: 0.1639436619718309 t: 0.34081101417541504\n",
      "Epoch 0: train Loss: 5.282312393188477 Score: 0.17111111111111113 t: 0.3409111499786377\n",
      "Epoch 0: test Loss: 1.6128599643707275 Score: 0.62 t: 0.30217838287353516\n",
      "Epoch 0: test Loss: 4.499418258666992 Score: 0.3381818181818182 t: 0.30753421783447266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|▎         | 1/30 [01:57<56:47, 117.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#==##==##==##==##==##==##==#epoch: 0#==##==##==##==##==##==##==#\n",
      "TEST ACC: 0.2691751085383502\n",
      "#==##==##==##==##==##==##==#time: 117.51021957397461#==##==##==##==##==##==##==#\n",
      "\n",
      "Epoch 1: train Loss: 4.7133402824401855 Score: 0.24 t: 0.34557294845581055\n",
      "Epoch 1: train Loss: 5.227316856384277 Score: 0.1890909090909091 t: 0.33803391456604004\n",
      "Epoch 1: train Loss: 5.433155536651611 Score: 0.19047619047619052 t: 0.3479311466217041\n",
      "Epoch 1: train Loss: 4.682940483093262 Score: 0.18838709677419355 t: 0.33509063720703125\n",
      "Epoch 1: train Loss: 5.223601818084717 Score: 0.18439024390243902 t: 0.3401308059692383\n",
      "Epoch 1: train Loss: 4.817471027374268 Score: 0.18941176470588236 t: 0.3399953842163086\n",
      "Epoch 1: train Loss: 4.834927082061768 Score: 0.19016393442622948 t: 0.34012269973754883\n",
      "Epoch 1: train Loss: 4.8107008934021 Score: 0.18816901408450704 t: 0.33916163444519043\n",
      "Epoch 1: train Loss: 5.915189266204834 Score: 0.18740740740740744 t: 0.3412156105041504\n",
      "Epoch 1: test Loss: 2.152493953704834 Score: 0.0 t: 0.3037700653076172\n",
      "Epoch 1: test Loss: 4.331676959991455 Score: 0.3472727272727273 t: 0.3088080883026123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|▋         | 2/30 [03:52<54:29, 116.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#==##==##==##==##==##==##==#epoch: 1#==##==##==##==##==##==##==#\n",
      "TEST ACC: 0.276410998552822\n",
      "#==##==##==##==##==##==##==#time: 115.03814697265625#==##==##==##==##==##==##==#\n",
      "\n",
      "Epoch 2: train Loss: 5.495522499084473 Score: 0.16 t: 0.3362751007080078\n",
      "Epoch 2: train Loss: 5.194477081298828 Score: 0.15272727272727274 t: 0.33989858627319336\n",
      "Epoch 2: train Loss: 5.3796210289001465 Score: 0.16095238095238096 t: 0.3397560119628906\n",
      "Epoch 2: train Loss: 4.5450825691223145 Score: 0.16387096774193546 t: 0.3385336399078369\n",
      "Epoch 2: train Loss: 4.328833103179932 Score: 0.16926829268292684 t: 0.3407893180847168\n",
      "Epoch 2: train Loss: 4.811136245727539 Score: 0.17568627450980392 t: 0.3399355411529541\n",
      "Epoch 2: train Loss: 5.261653900146484 Score: 0.18098360655737705 t: 0.3381927013397217\n",
      "Epoch 2: train Loss: 5.512700080871582 Score: 0.17943661971830988 t: 0.34123945236206055\n",
      "Epoch 2: train Loss: 5.651022434234619 Score: 0.17851851851851852 t: 0.34149837493896484\n",
      "Epoch 2: test Loss: 2.146533966064453 Score: 0.08 t: 0.30730271339416504\n",
      "Epoch 2: test Loss: 4.30919075012207 Score: 0.3563636363636364 t: 0.3128633499145508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 3/30 [05:19<48:31, 107.85s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#==##==##==##==##==##==##==#epoch: 2#==##==##==##==##==##==##==#\n",
      "TEST ACC: 0.28364688856729375\n",
      "#==##==##==##==##==##==##==#time: 87.02049446105957#==##==##==##==##==##==##==#\n",
      "\n",
      "Epoch 3: train Loss: 4.904222011566162 Score: 0.16 t: 0.3371870517730713\n",
      "Epoch 3: train Loss: 5.1126909255981445 Score: 0.17818181818181816 t: 0.34425783157348633\n",
      "Epoch 3: train Loss: 4.329429626464844 Score: 0.17714285714285719 t: 0.34143972396850586\n",
      "Epoch 3: train Loss: 5.68081521987915 Score: 0.17290322580645157 t: 0.3363606929779053\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = EncDec(embed_size, hidden_size, ques_vocab_size, ans_vocab_size, rnn_layers).to(device)\n",
    "model = FuseAttEncDec(embed_size, hidden_size, attention_size, \n",
    "                      ques_vocab_size, ans_vocab_size, num_layers, debug).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.get_parameters(), lr=learning_rate, momentum=momentum)\n",
    "# optimizer = torch.optim.Adam(model.get_parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = dataset.build_data_loader(train=True, args={'batch_size': batch_size})\n",
    "test_loader  = dataset.build_data_loader(test=True, args={'batch_size': batch_size})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "train_all_loss, train_all_labels, train_all_preds = [], [], []\n",
    "print(\"model built, start training.\")\n",
    "total_loss_over_epochs, scores_over_epochs = plotting.get_empty_stat_over_n_epoch_dictionaries()\n",
    "total_loss_over_epochs2, scores_over_epochs2 = plotting.get_empty_stat_over_n_epoch_dictionaries()\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    t0= time.time()\n",
    "    tr_loss, tr_labels, tr_preds = eval_model(data_loader = train_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = optimizer,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = True,\n",
    "                                     epoch       = epoch,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs,\n",
    "                                     scores_over_epochs     = scores_over_epochs)\n",
    "    \n",
    "    tr_loss, ts_labels, ts_preds = eval_model(data_loader = test_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = None,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = False,\n",
    "                                     epoch       = epoch,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs2,\n",
    "                                     scores_over_epochs     = scores_over_epochs2)\n",
    "    \n",
    "    \n",
    "    score = metrics.accuracy_score(ts_preds,ts_labels)\n",
    "#     total_loss_over_epochs['train_loss'].append(tr_loss)\n",
    "#     scores_over_epochs['train_scores'].append(train_scores)\n",
    "    \n",
    "#     if True:# or epoch%1 == 0:\n",
    "    print(\"\\n\"+\"#==#\"*7 + \"epoch: {}\".format(epoch) + \"#==#\"*7)\n",
    "    print('TEST ACC: {}'.format(score))\n",
    "    print(\"#==#\"*7 + \"time: {}\".format(time.time()-t0) + \"#==#\"*7 + \"\\n\")\n",
    "#         print(train_scores)\n",
    "#     plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(8,5))\n",
    "#     plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(8, 5), title=\"Loss\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    ts_loss, ts_labels, ts_preds = eval_model(data_loader = test_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = None,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = False,\n",
    "                                     epoch       = epoch,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs2,\n",
    "                                     scores_over_epochs     = scores_over_epochs2)\n",
    "    score = metrics.accuracy_score(ts_preds,ts_labels)\n",
    "    print(\"ACC: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_labels[0])\n",
    "print(tr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
