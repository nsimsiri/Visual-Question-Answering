{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, json, time\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import plotting\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import imread, img_data_2_mini_batch, imgs2batch\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from attention import Enc, Dec, EncDec\n",
    "from data_loader import VQADataSet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data_10000.pkl\n",
      "reading from ./data/data_10000.pkl\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "dataset_filename = \"./data/data_{}.pkl\".format(N)\n",
    "dataset = None\n",
    "print(dataset_filename)\n",
    "if (os.path.exists(dataset_filename)):\n",
    "    with open(dataset_filename, 'rb') as handle:\n",
    "        print(\"reading from \" + dataset_filename)\n",
    "        dataset = pickle.load(handle)\n",
    "else:\n",
    "    dataset = VQADataSet(Q=N)\n",
    "    with open(dataset_filename, 'wb') as handle:\n",
    "        print(\"writing to \" + dataset_filename)\n",
    "        pickle.dump(dataset, handle)\n",
    "\n",
    "assert(dataset is not None)\n",
    "def debug(v,q,a):\n",
    "    print('\\nV: {}\\nQ: {}\\nA: {}'.format(v.shape, q.shape, a.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size        = 300\n",
    "hidden_size       = 1024\n",
    "batch_size        = 50\n",
    "ques_vocab_size   = len(dataset.vocab['question'])\n",
    "c                 = len(dataset.vocab['answer'])\n",
    "num_layers        = 1\n",
    "n_epochs          = 20\n",
    "learning_rate     = 0.01\n",
    "momentum          = 0.98\n",
    "attention_size    = 512\n",
    "debug             = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EncDec(embed_size, hidden_size, attention_size, ques_vocab_size, c, num_layers, debug).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3891"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, criterion, optimizer, batch_size, training=False,\n",
    "              total_loss_over_epochs=[], scores_over_epochs=[]):\n",
    "    running_loss = 0.\n",
    "    final_labels, final_preds = [], []\n",
    "    if data_loader is None:\n",
    "        return\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    for i, minibatch in enumerate(data_loader):\n",
    "        # extract minibatch\n",
    "        t0 = time.time()\n",
    "        idxs, v, q, a, q_len = minibatch\n",
    "        \n",
    "        # convert torch's DataLoader output to proper format.\n",
    "        # torch gives a List[Tensor_1, ... ] where tensor has been transposed. \n",
    "        # batchify transposes back.`\n",
    "        v = v.to(device)\n",
    "        q = VQADataSet.batchify_questions(q).to(device)\n",
    "        a = a.to(device)\n",
    "                \n",
    "\n",
    "\n",
    "        logits = model(v, q, q_len)\n",
    "        \n",
    "        loss = F.nll_loss(logits, a)\n",
    "    \n",
    "        print(loss.item())\n",
    "        \n",
    "        if training and optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 50 shuffle: True\n",
      "batch_size: 50 shuffle: False\n",
      "8.272541999816895\n",
      "58.45856475830078\n",
      "104.03276062011719\n",
      "31.23953628540039\n",
      "13.028277397155762\n",
      "10.342129707336426\n",
      "8.001704216003418\n",
      "8.199151992797852\n",
      "136.3775634765625\n",
      "7.665517807006836\n",
      "7.716432571411133\n",
      "7.602004528045654\n",
      "7.748861312866211\n",
      "7.285489559173584\n",
      "7.5139312744140625\n",
      "7.4652557373046875\n",
      "7.795978546142578\n",
      "7.864872932434082\n",
      "7.552169322967529\n",
      "7.512897491455078\n",
      "7.867869853973389\n",
      "7.368079662322998\n",
      "7.559175491333008\n",
      "7.528205394744873\n",
      "7.679619312286377\n",
      "7.157935619354248\n",
      "7.1117730140686035\n",
      "7.139580726623535\n",
      "7.871028423309326\n",
      "7.655363082885742\n",
      "7.28419303894043\n",
      "7.3453688621521\n",
      "7.685708999633789\n",
      "8.440828323364258\n",
      "6.81636905670166\n",
      "8.257708549499512\n",
      "7.242302894592285\n",
      "6.717006206512451\n",
      "7.308776378631592\n",
      "7.483102321624756\n",
      "7.362673282623291\n",
      "6.996409893035889\n",
      "6.673761367797852\n",
      "8.49937629699707\n",
      "6.611283779144287\n",
      "7.940474033355713\n",
      "6.9325947761535645\n",
      "7.046403408050537\n",
      "7.1791582107543945\n",
      "7.475282192230225\n",
      "7.472904205322266\n",
      "7.952498912811279\n",
      "7.851423740386963\n",
      "7.492223739624023\n",
      "7.576979160308838\n",
      "7.897540092468262\n",
      "7.761585712432861\n",
      "7.570030689239502\n",
      "7.745840549468994\n",
      "7.279911994934082\n",
      "7.354633808135986\n",
      "7.514732837677002\n",
      "7.47421407699585\n",
      "7.012372970581055\n",
      "7.528630256652832\n",
      "6.868179798126221\n",
      "7.207397937774658\n",
      "7.224847316741943\n",
      "7.515045642852783\n",
      "7.367237567901611\n",
      "6.85800313949585\n",
      "7.401828765869141\n",
      "7.33146858215332\n",
      "7.506921291351318\n",
      "7.710651874542236\n",
      "7.184427261352539\n",
      "7.615417957305908\n",
      "7.148110389709473\n",
      "8.044024467468262\n",
      "7.6153459548950195\n",
      "7.222469329833984\n",
      "7.585855007171631\n",
      "7.11631965637207\n",
      "7.405855655670166\n",
      "7.159177780151367\n",
      "7.275153636932373\n",
      "7.0513739585876465\n",
      "7.354219913482666\n",
      "7.4120564460754395\n",
      "6.535202503204346\n",
      "7.4155707359313965\n",
      "6.985519886016846\n",
      "7.181707859039307\n",
      "6.810620307922363\n",
      "7.948052406311035\n",
      "7.581849575042725\n",
      "7.634669780731201\n",
      "7.679564952850342\n",
      "7.352716445922852\n",
      "7.5860819816589355\n",
      "7.633118152618408\n",
      "7.758924961090088\n",
      "7.331157207489014\n",
      "7.511055946350098\n",
      "7.517666816711426\n",
      "7.675327301025391\n",
      "6.994983673095703\n",
      "7.241710186004639\n",
      "7.5721564292907715\n",
      "7.541040420532227\n",
      "6.787006378173828\n",
      "6.659419536590576\n",
      "7.078800678253174\n",
      "8.12722396850586\n",
      "8.111656188964844\n",
      "7.5493645668029785\n",
      "7.650588512420654\n",
      "7.798557758331299\n",
      "7.581634044647217\n",
      "7.663773059844971\n",
      "7.468351364135742\n",
      "7.516674995422363\n",
      "7.6072001457214355\n",
      "7.973415374755859\n",
      "7.493389129638672\n",
      "6.9506072998046875\n",
      "7.169990062713623\n",
      "6.489950180053711\n",
      "7.108330249786377\n",
      "7.851052284240723\n",
      "7.149827480316162\n",
      "7.166536331176758\n",
      "7.302483081817627\n",
      "7.495721340179443\n",
      "7.426441669464111\n",
      "7.687912464141846\n",
      "7.363120079040527\n",
      "7.197990894317627\n",
      "7.248759746551514\n",
      "7.314180850982666\n",
      "7.106755256652832\n",
      "7.598698616027832\n",
      "6.8796610832214355\n",
      "6.418659210205078\n",
      "7.4438066482543945\n",
      "7.547986030578613\n",
      "7.2086286544799805\n",
      "7.184600353240967\n",
      "6.901037693023682\n",
      "7.484060287475586\n",
      "7.219815254211426\n",
      "7.381155014038086\n",
      "6.8132710456848145\n",
      "7.025679111480713\n",
      "7.049678802490234\n",
      "7.3941850662231445\n",
      "7.414140701293945\n",
      "7.148698329925537\n",
      "6.969816207885742\n",
      "7.133543014526367\n",
      "7.194584846496582\n",
      "6.9710564613342285\n",
      "7.15654182434082\n",
      "7.311004638671875\n",
      "7.132167339324951\n",
      "7.111413478851318\n",
      "7.227804660797119\n",
      "7.588673114776611\n",
      "6.738682746887207\n",
      "7.067357063293457\n",
      "6.522860527038574\n",
      "7.408138275146484\n",
      "7.29109001159668\n",
      "7.2587409019470215\n",
      "6.948969841003418\n",
      "6.562081813812256\n",
      "7.546016216278076\n",
      "7.314923286437988\n",
      "6.855950832366943\n",
      "7.391271591186523\n",
      "6.600915908813477\n",
      "7.457282066345215\n",
      "6.835853099822998\n",
      "6.956026554107666\n",
      "7.018314361572266\n",
      "7.224102020263672\n",
      "6.719177722930908\n",
      "6.542603969573975\n",
      "6.583493232727051\n",
      "7.24432373046875\n",
      "7.206957817077637\n",
      "6.806738376617432\n",
      "7.634916305541992\n",
      "7.323427677154541\n",
      "7.5541768074035645\n",
      "7.363616943359375\n",
      "7.412294864654541\n",
      "7.55806827545166\n",
      "7.1883111000061035\n",
      "7.185934543609619\n",
      "7.308796405792236\n",
      "7.47866678237915\n",
      "7.160318374633789\n",
      "7.5446577072143555\n",
      "7.234762668609619\n",
      "7.414831638336182\n",
      "7.240646839141846\n",
      "7.39988899230957\n",
      "7.382153511047363\n",
      "7.025464057922363\n",
      "7.421871185302734\n",
      "7.776637077331543\n",
      "7.284049034118652\n",
      "7.332332611083984\n",
      "7.579972743988037\n",
      "7.415745258331299\n",
      "7.740042209625244\n",
      "7.1763386726379395\n",
      "7.201016426086426\n",
      "7.592501163482666\n",
      "7.345726490020752\n",
      "7.17076301574707\n",
      "7.405198574066162\n",
      "7.696683406829834\n",
      "7.027622699737549\n",
      "7.339097499847412\n",
      "7.454089164733887\n",
      "7.308238506317139\n",
      "7.319810390472412\n",
      "7.3140339851379395\n",
      "7.277929782867432\n",
      "7.5521674156188965\n",
      "7.231635570526123\n",
      "7.338078022003174\n",
      "7.369863986968994\n",
      "7.151898384094238\n",
      "7.261580944061279\n",
      "7.1738810539245605\n",
      "7.327333927154541\n",
      "7.1993279457092285\n",
      "7.111979961395264\n",
      "7.135330677032471\n",
      "7.523146152496338\n",
      "6.955730438232422\n",
      "7.617393970489502\n",
      "7.162393569946289\n",
      "7.015167713165283\n",
      "7.3402557373046875\n",
      "7.261170864105225\n",
      "7.220952033996582\n",
      "7.336417198181152\n",
      "7.051003456115723\n",
      "7.267426013946533\n",
      "7.492359161376953\n",
      "7.399460315704346\n",
      "7.158847808837891\n",
      "7.168269157409668\n",
      "7.4042649269104\n",
      "7.2152557373046875\n",
      "7.151809215545654\n",
      "7.174088001251221\n",
      "6.864506244659424\n",
      "7.048101425170898\n",
      "7.183732986450195\n",
      "6.904603481292725\n",
      "7.203933238983154\n",
      "6.925243377685547\n",
      "7.145374774932861\n",
      "7.085666656494141\n",
      "7.2350077629089355\n",
      "6.989332675933838\n",
      "7.438681125640869\n",
      "7.2785210609436035\n",
      "7.091668128967285\n",
      "7.279027938842773\n",
      "7.662586688995361\n",
      "7.215324878692627\n",
      "7.142341136932373\n",
      "7.63414192199707\n",
      "7.101316452026367\n",
      "7.233163833618164\n",
      "7.331277370452881\n",
      "6.917909622192383\n",
      "7.1889214515686035\n",
      "6.792896747589111\n",
      "7.4399309158325195\n",
      "7.140171051025391\n",
      "6.873664379119873\n",
      "6.868210315704346\n",
      "6.982058048248291\n",
      "7.093095302581787\n",
      "7.358006477355957\n",
      "7.238797187805176\n",
      "7.082355976104736\n",
      "6.676739692687988\n",
      "7.341753005981445\n",
      "7.108809947967529\n",
      "6.943973541259766\n",
      "7.4998908042907715\n",
      "7.159399509429932\n",
      "7.1951069831848145\n",
      "6.8053154945373535\n",
      "6.636225700378418\n",
      "7.277200698852539\n",
      "6.9615349769592285\n",
      "7.189930438995361\n",
      "7.089817523956299\n",
      "7.406658172607422\n",
      "7.058765411376953\n",
      "6.996756553649902\n",
      "6.987735748291016\n",
      "6.956024169921875\n",
      "7.1105217933654785\n",
      "6.907526016235352\n",
      "6.959727764129639\n",
      "7.2605791091918945\n",
      "7.077490329742432\n",
      "7.326542377471924\n",
      "7.0441694259643555\n",
      "7.137579441070557\n",
      "7.279250621795654\n",
      "6.8927321434021\n",
      "7.005978584289551\n",
      "6.719459056854248\n",
      "6.953476428985596\n",
      "7.113461971282959\n",
      "6.928028106689453\n",
      "7.355097770690918\n",
      "6.918623447418213\n",
      "6.747987270355225\n",
      "7.307131767272949\n",
      "7.064304351806641\n",
      "7.3998589515686035\n",
      "7.082487106323242\n",
      "6.8214945793151855\n",
      "7.166994094848633\n",
      "6.948272705078125\n",
      "6.965177536010742\n",
      "6.649453163146973\n",
      "7.166399002075195\n",
      "7.321402072906494\n",
      "7.065381050109863\n",
      "7.120461940765381\n",
      "7.001186370849609\n",
      "6.984552383422852\n",
      "7.041683197021484\n",
      "7.042033672332764\n",
      "6.975555896759033\n",
      "6.974557399749756\n",
      "6.995975971221924\n",
      "6.910637378692627\n",
      "6.910943031311035\n",
      "6.748624324798584\n",
      "6.9222331047058105\n",
      "6.96102237701416\n",
      "7.143205165863037\n",
      "6.866599082946777\n",
      "6.690955638885498\n",
      "6.832157611846924\n",
      "6.786291122436523\n",
      "7.129373073577881\n",
      "6.905246734619141\n",
      "6.926914691925049\n",
      "7.194368362426758\n",
      "7.222288131713867\n",
      "7.114246368408203\n",
      "6.685206413269043\n",
      "6.9807257652282715\n",
      "7.041909217834473\n",
      "6.801357269287109\n",
      "7.1100568771362305\n",
      "6.898745059967041\n",
      "6.505654811859131\n",
      "7.422995090484619\n",
      "7.1333818435668945\n",
      "6.407225131988525\n",
      "6.962805271148682\n",
      "6.373820781707764\n",
      "6.566816806793213\n",
      "7.134042739868164\n",
      "7.165289878845215\n",
      "7.149529933929443\n",
      "6.866910457611084\n",
      "7.043154239654541\n",
      "6.437866687774658\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6800f6e32bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                      \u001b[0mtraining\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                      \u001b[0mtotal_loss_over_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss_over_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                      scores_over_epochs     = scores_over_epochs)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     train_scores = metrics.precision_recall_fscore_support(tr_labels,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = dataset.build_data_loader(train=True, args={'batch_size': batch_size})\n",
    "test_loader  = dataset.build_data_loader(test=True, args={'batch_size': batch_size})\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "train_all_loss, train_all_labels, train_all_preds = [], [], []\n",
    "\n",
    "total_loss_over_epochs, scores_over_epochs = plotting.get_empty_stat_over_n_epoch_dictionaries()\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    t0= time.time()\n",
    "    tr_loss, tr_labels, tr_preds = eval_model(data_loader = train_loader,\n",
    "                                     model       = model,\n",
    "                                     criterion   = criterion,\n",
    "                                     optimizer   = optimizer,\n",
    "                                     batch_size  = batch_size,\n",
    "                                     training    = True,\n",
    "                                     total_loss_over_epochs = total_loss_over_epochs,\n",
    "                                     scores_over_epochs     = scores_over_epochs)\n",
    "    \n",
    "#     train_scores = metrics.precision_recall_fscore_support(tr_labels,\n",
    "#                                                            tr_preds,\n",
    "#                                                            average='weighted')\n",
    "    \n",
    "#     total_loss_over_epochs['train_loss'].append(tr_loss)\n",
    "#     scores_over_epochs['train_scores'].append(train_scores)\n",
    "    \n",
    "#     if True:# or epoch%1 == 0:\n",
    "#         print(\"#==#\"*5 + \"epoch: {}\".format(epoch) + \"#==#\"*5)\n",
    "#         print(\"time: {}\".format(time.time()-t0))\n",
    "#         print(train_scores)\n",
    "#     plotting.plot_score_over_n_epochs(scores_over_epochs, score_type='precision', fig_size=(8,5))\n",
    "#     plotting.plot_loss_over_n_epochs(total_loss_over_epochs, fig_size=(8, 5), title=\"Loss\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(tr_labels))\n",
    "# print(type(tr_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tr_labels[0])\n",
    "# print(tr_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
